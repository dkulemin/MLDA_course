{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheyuriy/miniconda3/lib/python3.5/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['plt']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данных мало и можно загрузить их все в память."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./data/products_sentiment_train.tsv\", sep=\"\\t\", header=None, names=[\"text\", \"target\"])\n",
    "test_data = pd.read_csv(\"./data/products_sentiment_test.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Анализ данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на несколько примеров текстов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 . take around 10,000 640x480 pictures .\n",
      "from the internet , woodworking books , local stores and personal opinions the hitachi m12v 3-1 / 4 hp router time and time again came up to be the router of choice or best for its price .\n",
      "after multilple tries , one of the disks was finally recognized but video was poor and features not available . \n",
      "if you 've been listening to cd 's in your car for awhile , dealing with the drop-off in sound quality may take some getting used to .\n",
      "the door would not close . \n"
     ]
    }
   ],
   "source": [
    "print(train_data[\"text\"][0])\n",
    "print(train_data[\"text\"][7])\n",
    "print(train_data[\"text\"][100])\n",
    "print(train_data[\"text\"][1000])\n",
    "print(train_data[\"text\"][228])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что тексты предобработаны и токены достаточно корректно разделимы пробелами. Это упростит нашу задачу по токенизации в дальнейшем. Также можно заметить, что в тексте присутствуют числа и номера, а также сложные обозначения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем провести токенизацию простейшим способом (разбивая по пробелам). После этого посмотрим на наиболее и наименее употребимые слова в получаемом мешке слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data[\"text\"]\n",
    "Y = train_data[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheyuriy/miniconda3/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/cheyuriy/miniconda3/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/cheyuriy/miniconda3/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/cheyuriy/miniconda3/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens (naive split): 3973\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "vectorized_X = vectorizer.fit_transform(X)\n",
    "print(\"Total number of tokens (naive split):\", vectorized_X.shape[1])\n",
    "tokens = pd.DataFrame(vectorized_X.sum(axis=0).T, index=vectorizer.get_feature_names(), columns=[\"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The least frequent tokens:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lines</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funny</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funky</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reducer</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>functioning</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reduction</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reeves</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>refer</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>refill</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fumble</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             count\n",
       "lines            1\n",
       "funny            1\n",
       "funky            1\n",
       "reducer          1\n",
       "functioning      1\n",
       "reduction        1\n",
       "reeves           1\n",
       "refer            1\n",
       "refill           1\n",
       "fumble           1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The least frequent tokens:\")\n",
    "tokens.sort_values(by=\"count\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequent tokens:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <td>412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>1009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>2073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count\n",
       "you     383\n",
       "for     386\n",
       "this    409\n",
       "with    412\n",
       "of      471\n",
       "is      836\n",
       "it      855\n",
       "to      915\n",
       "and    1009\n",
       "the    2073"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The most frequent tokens:\")\n",
    "tokens.sort_values(by=\"count\").tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, среди наиболее частых слов все можно отнести к общеупотребительным стоп-словам. В дальнейшем мы попытаемся избавиться от них как от малозначащих. Для этого подсчитаем частоту появления таких слов среди всей массы слов и, определив некий порог, отсечём те, которые будем считать стоп-словами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>very</th>\n",
       "      <td>168</td>\n",
       "      <td>0.005173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>are</th>\n",
       "      <td>197</td>\n",
       "      <td>0.006066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>as</th>\n",
       "      <td>223</td>\n",
       "      <td>0.006867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my</th>\n",
       "      <td>239</td>\n",
       "      <td>0.007359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>251</td>\n",
       "      <td>0.007729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>259</td>\n",
       "      <td>0.007975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>but</th>\n",
       "      <td>264</td>\n",
       "      <td>0.008129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>have</th>\n",
       "      <td>271</td>\n",
       "      <td>0.008345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>315</td>\n",
       "      <td>0.009699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>364</td>\n",
       "      <td>0.011208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>383</td>\n",
       "      <td>0.011793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>386</td>\n",
       "      <td>0.011886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>409</td>\n",
       "      <td>0.012594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <td>412</td>\n",
       "      <td>0.012686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>471</td>\n",
       "      <td>0.014503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>836</td>\n",
       "      <td>0.025742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>855</td>\n",
       "      <td>0.026327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>915</td>\n",
       "      <td>0.028175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>1009</td>\n",
       "      <td>0.031069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>2073</td>\n",
       "      <td>0.063832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count        tf\n",
       "very    168  0.005173\n",
       "are     197  0.006066\n",
       "as      223  0.006867\n",
       "my      239  0.007359\n",
       "on      251  0.007729\n",
       "not     259  0.007975\n",
       "but     264  0.008129\n",
       "have    271  0.008345\n",
       "in      315  0.009699\n",
       "that    364  0.011208\n",
       "you     383  0.011793\n",
       "for     386  0.011886\n",
       "this    409  0.012594\n",
       "with    412  0.012686\n",
       "of      471  0.014503\n",
       "is      836  0.025742\n",
       "it      855  0.026327\n",
       "to      915  0.028175\n",
       "and    1009  0.031069\n",
       "the    2073  0.063832"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[\"tf\"] = tokens[\"count\"]/tokens[\"count\"].sum()\n",
    "tokens.sort_values(by=\"count\").tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'are',\n",
       " 'as',\n",
       " 'but',\n",
       " 'for',\n",
       " 'have',\n",
       " 'in',\n",
       " 'is',\n",
       " 'it',\n",
       " 'my',\n",
       " 'not',\n",
       " 'of',\n",
       " 'on',\n",
       " 'that',\n",
       " 'the',\n",
       " 'this',\n",
       " 'to',\n",
       " 'with',\n",
       " 'you']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = list(tokens.loc[tokens.tf > 0.006].index)\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве отсечки мы взяли частоту появления слова выше 0.006. Видно, что в эту группу попали слова, которые трудно отнести к определяющим тональность отзыва, а значит можно расчитывать, что их удаление не повредит модели.\n",
    "Однако стоит иметь ввиду, что эти слова могут быть достаточно важными при построении n-грам (например, слово not может кардинально поменять тональность рядом стоящего слова). Кроме того, мы могли бы использовать векторизацию через Tf-Idf, которая штрафует слишком частые слова, тем самым использование списка стоп-слов может быть и вовсе ненужным. Поэтому в дальнейшем мы будем использовать стоп-слова только при проверке моделей на основе частот, а не Tf-Idf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте попробуем визуализировать частоту появления слов в текстах разных тональностей. Вероятно, мы сможем определить некоторую закономерность."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorized_positive = vectorizer.fit_transform(list(train_data.loc[train_data.target == 1][\"text\"]))\n",
    "vectorized_positive = pd.DataFrame(vectorized_positive.sum(axis=0).T, index=vectorizer.get_feature_names(), columns=[\"positive\"])\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorized_negative = vectorizer.fit_transform(train_data.loc[train_data.target == 0][\"text\"])\n",
    "vectorized_negative = pd.DataFrame(vectorized_negative.sum(axis=0).T, index=vectorizer.get_feature_names(), columns=[\"negative\"])\n",
    "\n",
    "tokens = pd.merge(vectorized_negative, vectorized_positive, how=\"outer\", left_index=True, right_index=True)\n",
    "tokens = tokens.fillna(0)\n",
    "tokens[\"diff\"] = tokens[\"positive\"] - tokens[\"negative\"]\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "tokens[\"diff\"].sort_values().plot(title = \"Difference in frequency of words in mostly negative (left) and positive (right) reviews\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно из графика большинство слов одинаково часто встречаются как в положительных, так и в отрицательных отзывах. Лишь малая часть слов встречается значимо чаще в одной категории отзывов. Однако мы можем надеяться что классификитор, основанный на частоте слов, сможет успешно обучиться на них. Давайте посмотрим на эти слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most important tokens in positive reviews:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>156.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>42.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality</th>\n",
       "      <td>23.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>118.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phone</th>\n",
       "      <td>41.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>easy</th>\n",
       "      <td>10.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>very</th>\n",
       "      <td>39.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use</th>\n",
       "      <td>27.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>14.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>camera</th>\n",
       "      <td>33.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>102.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>134.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>118.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>170.0</td>\n",
       "      <td>301.0</td>\n",
       "      <td>131.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>8.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>137.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>382.0</td>\n",
       "      <td>533.0</td>\n",
       "      <td>151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <td>123.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>166.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>114.0</td>\n",
       "      <td>295.0</td>\n",
       "      <td>181.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>325.0</td>\n",
       "      <td>530.0</td>\n",
       "      <td>205.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>288.0</td>\n",
       "      <td>548.0</td>\n",
       "      <td>260.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>820.0</td>\n",
       "      <td>1253.0</td>\n",
       "      <td>433.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>287.0</td>\n",
       "      <td>722.0</td>\n",
       "      <td>435.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         negative  positive   diff\n",
       "you         156.0     227.0   71.0\n",
       "all          42.0     114.0   72.0\n",
       "quality      23.0     100.0   77.0\n",
       "in          118.0     197.0   79.0\n",
       "phone        41.0     120.0   79.0\n",
       "easy         10.0      99.0   89.0\n",
       "very         39.0     129.0   90.0\n",
       "use          27.0     125.0   98.0\n",
       "good         14.0     113.0   99.0\n",
       "camera       33.0     135.0  102.0\n",
       "for         134.0     252.0  118.0\n",
       "of          170.0     301.0  131.0\n",
       "great         8.0     145.0  137.0\n",
       "to          382.0     533.0  151.0\n",
       "with        123.0     289.0  166.0\n",
       "this        114.0     295.0  181.0\n",
       "it          325.0     530.0  205.0\n",
       "is          288.0     548.0  260.0\n",
       "the         820.0    1253.0  433.0\n",
       "and         287.0     722.0  435.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The most important tokens in positive reviews:\")\n",
    "tokens.sort_values(by=\"diff\").tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most important tokens in negative reviews:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>156.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>-53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>only</th>\n",
       "      <td>67.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>-39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <td>58.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>after</th>\n",
       "      <td>40.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>problem</th>\n",
       "      <td>38.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>they</th>\n",
       "      <td>53.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>norton</th>\n",
       "      <td>25.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>does</th>\n",
       "      <td>50.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>-16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>their</th>\n",
       "      <td>30.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>when</th>\n",
       "      <td>49.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>times</th>\n",
       "      <td>22.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>button</th>\n",
       "      <td>19.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sometimes</th>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>20.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buttons</th>\n",
       "      <td>19.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tried</th>\n",
       "      <td>15.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>off</th>\n",
       "      <td>21.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get</th>\n",
       "      <td>54.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>-11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>then</th>\n",
       "      <td>21.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>useless</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           negative  positive  diff\n",
       "not           156.0     103.0 -53.0\n",
       "only           67.0      28.0 -39.0\n",
       "would          58.0      32.0 -26.0\n",
       "after          40.0      16.0 -24.0\n",
       "problem        38.0      16.0 -22.0\n",
       "they           53.0      32.0 -21.0\n",
       "norton         25.0       4.0 -21.0\n",
       "does           50.0      34.0 -16.0\n",
       "their          30.0      15.0 -15.0\n",
       "when           49.0      35.0 -14.0\n",
       "times          22.0       8.0 -14.0\n",
       "button         19.0       6.0 -13.0\n",
       "sometimes      12.0       0.0 -12.0\n",
       "support        20.0       8.0 -12.0\n",
       "buttons        19.0       7.0 -12.0\n",
       "tried          15.0       3.0 -12.0\n",
       "off            21.0       9.0 -12.0\n",
       "get            54.0      43.0 -11.0\n",
       "then           21.0      11.0 -10.0\n",
       "useless        10.0       0.0 -10.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The most important tokens in negative reviews:\")\n",
    "tokens.sort_values(by=\"diff\").head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что многие слова были нами ранее отнесены к стоп-словам. Возможно по какой-то причине эти слова действительно могут определять тональность текста. Поэтому мы попробуем в дальнейшем как включать их в модель, так и исключать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline-модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перейдём непосредственно к проверке нескольких моделей. В качестве классификаторов попробуем использовать часто применяемые к таким задачам Multinomial Naive-Bayes classificator, Linear SVC, Ridge classificator.\n",
    "В качестве векторизаторов попробуем:\n",
    "* векторизатор по частоте слов\n",
    "    * с 1-1, 1-2 и 1-3 n-грамами\n",
    "    * со стоп словами и без\n",
    "    * с наивным токенизатором и токенизатором для анализа твитов из библиотеки nltk и стеммингом\n",
    "* векторизатор по Tf-Idf\n",
    "    * с 1-1, 1-2 и 1-3 n-грамами\n",
    "    * с наивным токенизатором и токенизатором для анализа твитов из библиотеки nltk и стеммингом\n",
    "    \n",
    "Для оценки наших кандидатов мы будем проводить кросс-валидацию на 10 свёртках. Метрикой будет выступать точность."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note, that you need to download parts of nltk to make this part work.\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "class TwitterTokenizerStemmer(object):\n",
    "    def __init__(self):\n",
    "        self.snowball_stemmer = SnowballStemmer(\"english\")\n",
    "        self.tokenizer = TweetTokenizer(preserve_case=False)\n",
    "    def __call__(self, doc):\n",
    "        tokenized_doc = self.tokenizer.tokenize(doc)\n",
    "        return \" \".join([self.snowball_stemmer.stem(x) for x in tokenized_doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to evaluate bunch of pipelines passed as argument\n",
    "def run_pipelines(pipelines, x, y, cv=10):\n",
    "    results = dict()\n",
    "    for (i, pipeline) in enumerate(pipelines):\n",
    "        pipeline_name = \":\".join([str(i), pipeline.steps[0][0], pipeline.steps[1][0]])\n",
    "        print(\"Evaluating\", pipeline_name)\n",
    "        results[pipeline_name] = np.mean(cross_val_score(pipeline, x, y, scoring=\"accuracy\", cv=cv, n_jobs=-1))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizers = [CountVectorizer(ngram_range=(1,1)), \n",
    "               CountVectorizer(ngram_range=(1,2)),\n",
    "               CountVectorizer(ngram_range=(1,3)),\n",
    "               CountVectorizer(ngram_range=(1,1), stop_words=stop_words), \n",
    "               CountVectorizer(ngram_range=(1,2), stop_words=stop_words),\n",
    "               CountVectorizer(ngram_range=(1,3), stop_words=stop_words),\n",
    "               CountVectorizer(ngram_range=(1,1), preprocessor=TwitterTokenizerStemmer()), \n",
    "               CountVectorizer(ngram_range=(1,2), preprocessor=TwitterTokenizerStemmer()),\n",
    "               CountVectorizer(ngram_range=(1,3), preprocessor=TwitterTokenizerStemmer()),\n",
    "               CountVectorizer(ngram_range=(1,1), stop_words=stop_words, preprocessor=TwitterTokenizerStemmer()), \n",
    "               CountVectorizer(ngram_range=(1,2), stop_words=stop_words, preprocessor=TwitterTokenizerStemmer()),\n",
    "               CountVectorizer(ngram_range=(1,3), stop_words=stop_words, preprocessor=TwitterTokenizerStemmer()),\n",
    "               TfidfVectorizer(ngram_range=(1,1)),\n",
    "               TfidfVectorizer(ngram_range=(1,2)),\n",
    "               TfidfVectorizer(ngram_range=(1,3)),\n",
    "               TfidfVectorizer(ngram_range=(1,1), preprocessor=TwitterTokenizerStemmer()),\n",
    "               TfidfVectorizer(ngram_range=(1,2), preprocessor=TwitterTokenizerStemmer()),\n",
    "               TfidfVectorizer(ngram_range=(1,3), preprocessor=TwitterTokenizerStemmer()),\n",
    "               TfidfVectorizer(ngram_range=(1,1), stop_words=stop_words),\n",
    "               TfidfVectorizer(ngram_range=(1,2), stop_words=stop_words),\n",
    "               TfidfVectorizer(ngram_range=(1,3), stop_words=stop_words),\n",
    "               TfidfVectorizer(ngram_range=(1,1), stop_words=stop_words, preprocessor=TwitterTokenizerStemmer()),\n",
    "               TfidfVectorizer(ngram_range=(1,2), stop_words=stop_words, preprocessor=TwitterTokenizerStemmer()),\n",
    "               TfidfVectorizer(ngram_range=(1,3), stop_words=stop_words, preprocessor=TwitterTokenizerStemmer())\n",
    "              ]\n",
    "models = [LinearSVC(), \n",
    "          MultinomialNB(), \n",
    "          RidgeClassifier()\n",
    "         ]\n",
    "pipelines_to_evaluate = list(\n",
    "    map(lambda x: make_pipeline(*x), \n",
    "        product(vectorizers, models))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 4.29 µs\n",
      "Evaluating 0:countvectorizer:linearsvc\n",
      "Evaluating 1:countvectorizer:multinomialnb\n",
      "Evaluating 2:countvectorizer:ridgeclassifier\n",
      "Evaluating 3:countvectorizer:linearsvc\n",
      "Evaluating 4:countvectorizer:multinomialnb\n",
      "Evaluating 5:countvectorizer:ridgeclassifier\n",
      "Evaluating 6:countvectorizer:linearsvc\n",
      "Evaluating 7:countvectorizer:multinomialnb\n",
      "Evaluating 8:countvectorizer:ridgeclassifier\n",
      "Evaluating 9:countvectorizer:linearsvc\n",
      "Evaluating 10:countvectorizer:multinomialnb\n",
      "Evaluating 11:countvectorizer:ridgeclassifier\n",
      "Evaluating 12:countvectorizer:linearsvc\n",
      "Evaluating 13:countvectorizer:multinomialnb\n",
      "Evaluating 14:countvectorizer:ridgeclassifier\n",
      "Evaluating 15:countvectorizer:linearsvc\n",
      "Evaluating 16:countvectorizer:multinomialnb\n",
      "Evaluating 17:countvectorizer:ridgeclassifier\n",
      "Evaluating 18:countvectorizer:linearsvc\n",
      "Evaluating 19:countvectorizer:multinomialnb\n",
      "Evaluating 20:countvectorizer:ridgeclassifier\n",
      "Evaluating 21:countvectorizer:linearsvc\n",
      "Evaluating 22:countvectorizer:multinomialnb\n",
      "Evaluating 23:countvectorizer:ridgeclassifier\n",
      "Evaluating 24:countvectorizer:linearsvc\n",
      "Evaluating 25:countvectorizer:multinomialnb\n",
      "Evaluating 26:countvectorizer:ridgeclassifier\n",
      "Evaluating 27:countvectorizer:linearsvc\n",
      "Evaluating 28:countvectorizer:multinomialnb\n",
      "Evaluating 29:countvectorizer:ridgeclassifier\n",
      "Evaluating 30:countvectorizer:linearsvc\n",
      "Evaluating 31:countvectorizer:multinomialnb\n",
      "Evaluating 32:countvectorizer:ridgeclassifier\n",
      "Evaluating 33:countvectorizer:linearsvc\n",
      "Evaluating 34:countvectorizer:multinomialnb\n",
      "Evaluating 35:countvectorizer:ridgeclassifier\n",
      "Evaluating 36:tfidfvectorizer:linearsvc\n",
      "Evaluating 37:tfidfvectorizer:multinomialnb\n",
      "Evaluating 38:tfidfvectorizer:ridgeclassifier\n",
      "Evaluating 39:tfidfvectorizer:linearsvc\n",
      "Evaluating 40:tfidfvectorizer:multinomialnb\n",
      "Evaluating 41:tfidfvectorizer:ridgeclassifier\n",
      "Evaluating 42:tfidfvectorizer:linearsvc\n",
      "Evaluating 43:tfidfvectorizer:multinomialnb\n",
      "Evaluating 44:tfidfvectorizer:ridgeclassifier\n",
      "Evaluating 45:tfidfvectorizer:linearsvc\n",
      "Evaluating 46:tfidfvectorizer:multinomialnb\n",
      "Evaluating 47:tfidfvectorizer:ridgeclassifier\n",
      "Evaluating 48:tfidfvectorizer:linearsvc\n",
      "Evaluating 49:tfidfvectorizer:multinomialnb\n",
      "Evaluating 50:tfidfvectorizer:ridgeclassifier\n",
      "Evaluating 51:tfidfvectorizer:linearsvc\n",
      "Evaluating 52:tfidfvectorizer:multinomialnb\n",
      "Evaluating 53:tfidfvectorizer:ridgeclassifier\n",
      "Evaluating 54:tfidfvectorizer:linearsvc\n",
      "Evaluating 55:tfidfvectorizer:multinomialnb\n",
      "Evaluating 56:tfidfvectorizer:ridgeclassifier\n",
      "Evaluating 57:tfidfvectorizer:linearsvc\n",
      "Evaluating 58:tfidfvectorizer:multinomialnb\n",
      "Evaluating 59:tfidfvectorizer:ridgeclassifier\n",
      "Evaluating 60:tfidfvectorizer:linearsvc\n",
      "Evaluating 61:tfidfvectorizer:multinomialnb\n",
      "Evaluating 62:tfidfvectorizer:ridgeclassifier\n",
      "Evaluating 63:tfidfvectorizer:linearsvc\n",
      "Evaluating 64:tfidfvectorizer:multinomialnb\n",
      "Evaluating 65:tfidfvectorizer:ridgeclassifier\n",
      "Evaluating 66:tfidfvectorizer:linearsvc\n",
      "Evaluating 67:tfidfvectorizer:multinomialnb\n",
      "Evaluating 68:tfidfvectorizer:ridgeclassifier\n",
      "Evaluating 69:tfidfvectorizer:linearsvc\n",
      "Evaluating 70:tfidfvectorizer:multinomialnb\n",
      "Evaluating 71:tfidfvectorizer:ridgeclassifier\n"
     ]
    }
   ],
   "source": [
    "#it will take time\n",
    "np.random.seed(1)\n",
    "%time\n",
    "results = run_pipelines(pipelines_to_evaluate, X, Y)\n",
    "%time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим подробнее на наиболее успешные комбинации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: multinomialnb with countvectorizer (ngram_range: (1, 1) preprocessor: <__main__.TwitterTokenizerStemmer object at 0x7fcbd4594438> stopwords: True ) accuracy: 0.7944682617065427\n",
      "Model: multinomialnb with countvectorizer (ngram_range: (1, 1) preprocessor: <__main__.TwitterTokenizerStemmer object at 0x7fcbd4612780> stopwords: False ) accuracy: 0.7939883372084303\n",
      "Model: linearsvc with tfidfvectorizer (ngram_range: (1, 3) preprocessor: <__main__.TwitterTokenizerStemmer object at 0x7fcbd4594b70> stopwords: False ) accuracy: 0.793480799519988\n",
      "Model: ridgeclassifier with tfidfvectorizer (ngram_range: (1, 2) preprocessor: <__main__.TwitterTokenizerStemmer object at 0x7fcbd4594a20> stopwords: False ) accuracy: 0.7914732868321709\n",
      "Model: ridgeclassifier with tfidfvectorizer (ngram_range: (1, 2) preprocessor: <__main__.TwitterTokenizerStemmer object at 0x7fcbd4594f60> stopwords: True ) accuracy: 0.790948236205905\n"
     ]
    }
   ],
   "source": [
    "for r in sorted(results.items(), key=lambda x: -x[1])[0:5]:\n",
    "    i = int(r[0].split(\":\")[0])\n",
    "    pipeline = pipelines_to_evaluate[i]\n",
    "    vectorizer = pipeline.steps[0][0]\n",
    "    ngram_range = pipeline.steps[0][1].ngram_range\n",
    "    preprocessor = pipeline.steps[0][1].preprocessor\n",
    "    stopwords = bool(pipeline.steps[0][1].stop_words)\n",
    "    model = pipeline.steps[1][0]\n",
    "    print(\"Model:\", model,\n",
    "          \"with\",vectorizer,\n",
    "          \"(ngram_range:\",ngram_range,\n",
    "          \"preprocessor:\",preprocessor,\n",
    "          \"stopwords:\",stopwords,\")\", \"accuracy:\",r[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интересное наблюдение, что Multinomial Naive-Bayes показал наилучший результат только на униграмах, причём наличие стоп-слов добавило точности. Обратная ситуация наблюдается для других моделей: они показали себя лучше в условиях наличия как биграмм, так и триграмм, а также не так очевидна польза стоп-слов.\n",
    "В качестве дальнейших моделей для улучшения возьмём:\n",
    "* Multinomial Naive-Bayes c частотным векторизатором на униграммах с фильтрацией стоп-слов\n",
    "* Linear SVC с Tf-Idf векторизатором на 1-3-граммах без фильтрации стоп-слов\n",
    "\n",
    "В обоих случаях будем использовать наш сложный токенизатор со стеммингом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive-Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Остановим внимание на том факте, что добавление стоп-слов улучшило результат данного классификатора. Попробуем расширить список таких слов, но при этом вручную сохраним те слова, которые на наш взгляд могут быть важны для определения тональности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorized_X = vectorizer.fit_transform(X)\n",
    "tokens = pd.DataFrame(vectorized_X.sum(axis=0).T, index=vectorizer.get_feature_names(), columns=[\"count\"])\n",
    "tokens[\"tf\"] = tokens[\"count\"]/tokens[\"count\"].sum()\n",
    "\n",
    "vectorizers = list()\n",
    "list_stop_words = list()\n",
    "df_thresholds_to_test = np.linspace(0.0015, 0.006, 10)\n",
    "for threshold in df_thresholds_to_test:\n",
    "    list_stop_words += [set(tokens.loc[tokens.tf > threshold].index) - set([\"but\", \"easy\", \"features\", \"good\", \"great\", \"like\", \"more\", \"not\", \"no\", \"only\", \"quality\", \"than\", \"very\", \"well\", \"work\", \"zen\", \"best\", \"better\", \"creative\", \"excellent\", \"just\", \"little\", \"love\", \"much\", \"price\", \"problem\", \"problems\", \"small\", \"some\"])]\n",
    "    vectorizers += [CountVectorizer(ngram_range=(1,1), stop_words=list_stop_words[-1], preprocessor=TwitterTokenizerStemmer())]\n",
    "    \n",
    "models = [MultinomialNB()]\n",
    "\n",
    "pipelines_to_evaluate = list(\n",
    "    map(lambda x: make_pipeline(*x), \n",
    "        product(vectorizers, models))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 4.05 µs\n",
      "Evaluating 0:countvectorizer:multinomialnb\n",
      "Evaluating 1:countvectorizer:multinomialnb\n",
      "Evaluating 2:countvectorizer:multinomialnb\n",
      "Evaluating 3:countvectorizer:multinomialnb\n",
      "Evaluating 4:countvectorizer:multinomialnb\n",
      "Evaluating 5:countvectorizer:multinomialnb\n",
      "Evaluating 6:countvectorizer:multinomialnb\n",
      "Evaluating 7:countvectorizer:multinomialnb\n",
      "Evaluating 8:countvectorizer:multinomialnb\n",
      "Evaluating 9:countvectorizer:multinomialnb\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 4.05 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "results = run_pipelines(pipelines_to_evaluate, X, Y)\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal df threshold: 0.005\n",
      "Optimal stopword list: {'that', 'to', 'can', 'is', 'in', 'and', 'this', 'of', 'it', 'my', 'with', 'have', 'for', 'as', 'are', 'camera', 'the', 'you', 'on'}\n",
      "Accuracy: 0.7969709242731069\n"
     ]
    }
   ],
   "source": [
    "best_pipeline_num = int(sorted(results.items(), key=lambda x: -x[1])[0][0].split(\":\")[0])\n",
    "optimal_stop_words = list(pipelines_to_evaluate[best_pipeline_num].steps[0][1].stop_words)\n",
    "print(\"Optimal df threshold:\",df_thresholds_to_test[best_pipeline_num])\n",
    "print(\"Optimal stopword list:\", pipelines_to_evaluate[best_pipeline_num].steps[0][1].stop_words)\n",
    "print(\"Accuracy:\", sorted(results.items(), key=lambda x: -x[1])[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее мы попробуем подогнать прочие параметры токенизатора и модели путём поиска по сетке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "pipe = Pipeline(steps=[('vectorizer', CountVectorizer(ngram_range=(1,1), preprocessor=TwitterTokenizerStemmer(), stop_words=optimal_stop_words)), \n",
    "                       ('model', MultinomialNB())])\n",
    "param_grid = {\n",
    "    'vectorizer__min_df': np.linspace(0.0,0.1,num=11),\n",
    "    'vectorizer__max_df': np.linspace(0.6,0.8,num=11),\n",
    "    'model__alpha': np.linspace(1.0,10.0,num=11)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GridSearchCV(pipe, param_grid, iid=True, cv=10, return_train_score=False, scoring=\"accuracy\", n_jobs=-1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1331 candidates, totalling 13310 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   24.7s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   57.7s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed:  9.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4984 tasks      | elapsed: 11.1min\n",
      "[Parallel(n_jobs=-1)]: Done 6034 tasks      | elapsed: 13.4min\n",
      "[Parallel(n_jobs=-1)]: Done 7184 tasks      | elapsed: 15.9min\n",
      "[Parallel(n_jobs=-1)]: Done 8434 tasks      | elapsed: 18.6min\n",
      "[Parallel(n_jobs=-1)]: Done 9784 tasks      | elapsed: 21.6min\n",
      "[Parallel(n_jobs=-1)]: Done 11234 tasks      | elapsed: 24.8min\n",
      "[Parallel(n_jobs=-1)]: Done 12784 tasks      | elapsed: 28.2min\n",
      "[Parallel(n_jobs=-1)]: Done 13310 out of 13310 | elapsed: 29.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1),\n",
       "        preprocessor=<__main__.TwitterToken...izer=None, vocabulary=None)), ('model', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'vectorizer__max_df': array([0.6 , 0.62, 0.64, 0.66, 0.68, 0.7 , 0.72, 0.74, 0.76, 0.78, 0.8 ]), 'model__alpha': array([ 1. ,  1.9,  2.8,  3.7,  4.6,  5.5,  6.4,  7.3,  8.2,  9.1, 10. ]), 'vectorizer__min_df': array([0.  , 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "       scoring='accuracy', verbose=True)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7985"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__alpha': 1.9, 'vectorizer__max_df': 0.6, 'vectorizer__min_df': 0.0}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Особого выигрыша от подгонки параметров не получили, однако точность на кроссвалидации очень близка к требуемому порогу. Соберём модель ещё раз, получим предсказания для тестового набора данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7959883622090553"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,1), preprocessor=TwitterTokenizerStemmer(), stop_words=optimal_stop_words, max_df=0.6, min_df=0.0)\n",
    "vectorized_X = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB(alpha=1.9)\n",
    "clf.fit(vectorized_X, Y)\n",
    "results = clf.predict(vectorizer.transform(test_data[\"text\"]))\n",
    "test_results = pd.DataFrame(results,columns=[\"y\"])\n",
    "test_results.to_csv(\"multinomialnb.csv\", columns=[\"y\"], index_label=\"Id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем просто запустить подгонку параметров по сетке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[('vectorizer', TfidfVectorizer(preprocessor=TwitterTokenizerStemmer(), ngram_range=(1,3))), \n",
    "                       ('model', LinearSVC())])\n",
    "\n",
    "param_grid = {\n",
    "    'vectorizer__max_df': np.linspace(0.1,0.6,26),\n",
    "    'model__C': np.linspace(0.7,0.9,11)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GridSearchCV(pipe, param_grid, iid=True, cv=10, scoring=\"accuracy\", n_jobs=-1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 286 candidates, totalling 2860 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   12.3s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  9.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed: 13.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2860 out of 2860 | elapsed: 15.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 3), norm='l2',\n",
       "        preprocessor=<__main__.T...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'vectorizer__max_df': array([0.1 , 0.12, 0.14, 0.16, 0.18, 0.2 , 0.22, 0.24, 0.26, 0.28, 0.3 ,\n",
       "       0.32, 0.34, 0.36, 0.38, 0.4 , 0.42, 0.44, 0.46, 0.48, 0.5 , 0.52,\n",
       "       0.54, 0.56, 0.58, 0.6 ]), 'model__C': array([0.7 , 0.72, 0.74, 0.76, 0.78, 0.8 , 0.82, 0.84, 0.86, 0.88, 0.9 ])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=True)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7995"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__C': 0.76, 'vectorizer__max_df': 0.14}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(preprocessor=TwitterTokenizerStemmer(), ngram_range=(1,3), max_df=0.14)\n",
    "vectorized_X = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC(C=0.76)\n",
    "clf.fit(vectorized_X, Y)\n",
    "results = clf.predict(vectorizer.transform(test_data[\"text\"]))\n",
    "test_results = pd.DataFrame(results,columns=[\"y\"])\n",
    "test_results.to_csv(\"linearcsv.csv\", columns=[\"y\"], index_label=\"Id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы\n",
    "Хотя при первом приближении Multinomial Naive Bayes дал точность выше, но всё же более тонкая настройка сделала предпочтительнее Linear SVC. Это подтвердилось результатами тетсирования на отложенной выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
