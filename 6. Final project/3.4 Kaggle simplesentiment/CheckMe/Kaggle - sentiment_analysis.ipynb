{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment-analysis on goods (simple version)\n",
    "https://inclass.kaggle.com/c/product-reviews-sentiment-analysis-light/data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import liblraries, dataset, browse it and look at any features here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('products_sentiment_train.tsv', delimiter  = '\t',header = -1)\n",
    "data.columns = ('text','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2 . take around 10,000 640x480 pictures .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i downloaded a trial version of computer assoc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the wrt54g plus the hga7t is a perfect solutio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i dont especially like how music files are uns...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i was using the cheapie pail ... and it worked...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0          2 . take around 10,000 640x480 pictures .      1\n",
       "1  i downloaded a trial version of computer assoc...      1\n",
       "2  the wrt54g plus the hga7t is a perfect solutio...      1\n",
       "3  i dont especially like how music files are uns...      0\n",
       "4  i was using the cheapie pail ... and it worked...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 2000 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.637"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "63,7% of its are positive, others - negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\veselov.a.AK\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import functions\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 53.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# making vectorization, transform\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(data['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3973"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many words do we have?\n",
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making pipeline to next working\n",
    "pipeline = Pipeline([('vect', vectorizer), ('LR', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ger first estimation of basic algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7684956843480272"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(pipeline,data['text'],data['label'],cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([('vect', vectorizer), ('LR', LogisticRegression())])\n",
    "pipeline.fit(data['text'],data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking for a most valuables words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.033965186133173 great\n",
      "1.8197688401247354 love\n",
      "1.565612204473604 excellent\n",
      "1.5127351324238094 easy\n",
      "1.3966729722929636 perfect\n"
     ]
    }
   ],
   "source": [
    "coeeficients = pipeline.named_steps['LR'].coef_[0]\n",
    "\n",
    "idx = (-coeeficients).argsort()[:5]\n",
    "for i in idx:\n",
    "    print(coeeficients[i], pipeline.named_steps['vect'].get_feature_names()[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive words are most valuable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking what is better - TfidfVectorizer or Countvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7684956843480272\n",
      "0.007634111236534462\n"
     ]
    }
   ],
   "source": [
    "pipeline_a = Pipeline([('vect', vectorizer), ('LR', LogisticRegression())])\n",
    "print(cross_val_score(pipeline_a,data['text'],data['label'],cv=5).mean())\n",
    "print(cross_val_score(pipeline_a,data['text'],data['label'],cv=5).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7665031843949025\n",
      "0.011066947966561875\n"
     ]
    }
   ],
   "source": [
    "pipeline_b = Pipeline([('vect', TfidfVectorizer()), ('LR', LogisticRegression())])\n",
    "print(cross_val_score(pipeline_b,data['text'],data['label'],cv=5).mean())\n",
    "print(cross_val_score(pipeline_b,data['text'],data['label'],cv=5).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if min_df will help us to improve Classificator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_df =  0 0.7684956843480272\n",
      "min_df =  1 0.7684956843480272\n",
      "min_df =  2 0.7699844436527729\n",
      "min_df =  3 0.7654894311839449\n",
      "min_df =  4 0.7609906655666598\n",
      "min_df =  5 0.7639856780354878\n",
      "min_df =  6 0.7584906593166207\n",
      "min_df =  7 0.755991937449609\n",
      "min_df =  8 0.7500019062619141\n",
      "min_df =  9 0.7525169313558209\n"
     ]
    }
   ],
   "source": [
    "for a in range(0,10):\n",
    "    pipeline = Pipeline([('vect', CountVectorizer(min_df=a)), ('LR', LogisticRegression())])\n",
    "    print('min_df = ', a, cross_val_score(pipeline,data['text'],data['label'],cv=5).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min_df = 2 is the best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking which classificator is the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7699844436527729 0.742506856292852 0.7354681216757605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\veselov.a.AK\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\veselov.a.AK\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\veselov.a.AK\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\veselov.a.AK\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\veselov.a.AK\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "pipeline_a = Pipeline([('vect', CountVectorizer(min_df=2)), ('LR', LogisticRegression())])\n",
    "pipeline_b = Pipeline([('vect', CountVectorizer(min_df=2)), ('LR', LinearSVC())])\n",
    "pipeline_c = Pipeline([('vect', CountVectorizer(min_df=2)), ('LR', SGDClassifier())])\n",
    "\n",
    "print(cross_val_score(pipeline_a,data['text'],data['label'],cv=5).mean(), cross_val_score(pipeline_b,data['text'],data['label'],cv=5).mean(), cross_val_score(pipeline_c,data['text'],data['label'],cv=5).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogisticRegression is the best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to use stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\veselov.a.AK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_english = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer1 = CountVectorizer(stop_words = 'english',min_df=2)\n",
    "vectorizer2 = CountVectorizer(stop_words = stop_english,min_df=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7459943687148044 0.750003121894512\n"
     ]
    }
   ],
   "source": [
    "pipeline_a = Pipeline([('vect', vectorizer1), ('LR', LogisticRegression())])\n",
    "pipeline_b = Pipeline([('vect', vectorizer2), ('LR', LogisticRegression())])\n",
    "print(cross_val_score(pipeline_a,data['text'],data['label'],cv=5).mean(), cross_val_score(pipeline_b,data['text'],data['label'],cv=5).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7664956780979881\n"
     ]
    }
   ],
   "source": [
    "vectorizer1 = CountVectorizer(ngram_range=(1, 2),min_df=2)\n",
    "pipeline_a = Pipeline([('vect', vectorizer1), ('LR', LogisticRegression())])\n",
    "print(cross_val_score(pipeline_a,data['text'],data['label'],cv=5).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7529956343477148\n"
     ]
    }
   ],
   "source": [
    "vectorizer2 = CountVectorizer(ngram_range=(3, 5),analyzer='char_wb', min_df=2)\n",
    "pipeline_2 = Pipeline([('vect', vectorizer2), ('LR', LogisticRegression())])\n",
    "print(cross_val_score(pipeline_2,data['text'],data['label'],cv=5).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model was simple min_df=2 without using stop_words parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7699844436527729\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([('vect', CountVectorizer(min_df=2)), ('LR', LogisticRegression())])\n",
    "print(cross_val_score(pipeline,data['text'],data['label'],cv=5).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500 0.7719882155513471\n",
      "1501 0.7714882155513473\n",
      "1502 0.7714882155513473\n",
      "1503 0.7719882155513471\n",
      "1504 0.7719882155513471\n",
      "1505 0.7714882155513473\n",
      "1506 0.7714882155513473\n",
      "1507 0.7714882155513473\n",
      "1508 0.7714882155513473\n",
      "1509 0.7719882155513472\n",
      "1510 0.7719882155513472\n",
      "1511 0.7719882155513472\n",
      "1512 0.7719882155513472\n",
      "1513 0.7719882155513472\n",
      "1514 0.7719882155513472\n",
      "1515 0.7719882155513472\n",
      "1516 0.7714869624185152\n",
      "1517 0.7714869624185152\n",
      "1518 0.770986962418515\n",
      "1519 0.770986962418515\n",
      "1520 0.770986962418515\n",
      "1521 0.770986962418515\n",
      "1522 0.770986962418515\n",
      "1523 0.770986962418515\n",
      "1524 0.770986962418515\n",
      "1525 0.770986962418515\n",
      "1526 0.770986962418515\n",
      "1527 0.770986962418515\n",
      "1528 0.7719857155357222\n",
      "1529 0.7719857155357222\n"
     ]
    }
   ],
   "source": [
    "for a in np.arange(1500, 1530, 1):\n",
    "    pipeline = Pipeline([('vect', CountVectorizer(min_df=2,max_features=a)), ('LR', LogisticRegression())])\n",
    "    print(a, cross_val_score(pipeline,data['text'],data['label'],cv=5).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, 1500 is the best))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7719882155513471\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([('vect', CountVectorizer(min_df=2,max_features=1500)), ('LR', LogisticRegression(penalty='l2'))])\n",
    "print(cross_val_score(pipeline,data['text'],data['label'],cv=5).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv('products_sentiment_test.tsv', delimiter  = '\t',header = 0)\n",
    "data_test.columns = ('...','text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor=LogisticRegression(penalty='l2')\n",
    "vectorizer=CountVectorizer(min_df=2,max_features=1500)\n",
    "\n",
    "CV=vectorizer.fit_transform(data['text'])\n",
    "LogReg=regressor.fit(CV, data['label'])\n",
    "\n",
    "predict=LogReg.predict(vectorizer.transform(data_test['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      so , why the small digital elph , rather than ...\n",
       "1      3/4 way through the first disk we played on it...\n",
       "2      better for the zen micro is outlook compatibil...\n",
       "3        6 . play gameboy color games on it with goboy .\n",
       "4      likewise , i 've heard norton 2004 professiona...\n",
       "5      mine was 2 weeks old and i chucked it in the t...\n",
       "6      i find it very stable and comfortable to use ,...\n",
       "7      styling / ergonomics : the keys are small , wh...\n",
       "8      at first i thought it is only a isolated incid...\n",
       "9                              - light , compact design \n",
       "10     although the sd500 takes great quality photos ...\n",
       "11     after years with that carrier 's expensive pla...\n",
       "12     i 've only had this camera for a few days , bu...\n",
       "13     the user support service isn 't too great eith...\n",
       "14     the headphones have great sound and durability...\n",
       "15     file transfers are fast , nearly a song per se...\n",
       "16     creative are * the * sound people for computer...\n",
       "17     unfortunately , i sold it exactly a month later .\n",
       "18                                 i love this camera . \n",
       "19     while i like the performance of the phone in e...\n",
       "20      battery life is good for a phone of this size . \n",
       "21     as far as dealing with network access of other...\n",
       "22     the scroll button is sort of annoying to push ...\n",
       "23     pros.1.speed 2 . signal strength 3.advanced co...\n",
       "24     4mp , 4x optical zoom , takes absolutely beaut...\n",
       "25     i did have to put a little work into renaming ...\n",
       "26     i bought this diaper pail when my son was just...\n",
       "27     but i had to give you four because the service...\n",
       "28     i can connect to my computer via bluetooth to ...\n",
       "29     the champ did not start letting a little odor ...\n",
       "                             ...                        \n",
       "470       excellent range of metering options as well . \n",
       "471    all-in-all , i believe this is arguably the be...\n",
       "472               proven canon built quality and lens . \n",
       "473    i found it intuitive to use ( i did n't read a...\n",
       "474    i am a picture fanatic so i consider myself pi...\n",
       "475    i got this camera for an unbelievably low pric...\n",
       "476    bottom line , this camera will give you a lot ...\n",
       "477        excellent sound i mean the player has juice .\n",
       "478    however after just one month my headphone jack...\n",
       "479    it only read the dvds 10 % of the time , other...\n",
       "480    my music demands are pretty intense and this h...\n",
       "481    his is what i get for my $ 200 on this mp3 pla...\n",
       "482    navigation system - the zen xtra uses id3 tags...\n",
       "483    the only two minor issues i have with the came...\n",
       "484    other tagging problems result ( i think ) from...\n",
       "485    the use of 65536 ( 16bit ) colors ensures you ...\n",
       "486    if you strictly use the lcd and not the view f...\n",
       "487    this fixes some glitches such as the player fr...\n",
       "488    if you buying this player to use with napster ...\n",
       "489    this model does have the traditional key arran...\n",
       "490       even with the waterproof housing it is small .\n",
       "491    this version of internet security 2004 , is be...\n",
       "492    i recently had to run a 3 / 4 \" straight bit t...\n",
       "493    transfer through windows explorer - windows re...\n",
       "494    but like i stated , its better then a trash ca...\n",
       "495    i took perfect care of this player and still i...\n",
       "496                    it 's a very intuitive program . \n",
       "497    the only drawback is the viewfinder is slightl...\n",
       "498     it films 10 second video , for crying out loud .\n",
       "499                       everything shines of quality .\n",
       "Name: text, Length: 500, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "save=[]\n",
    "for i in range(len(predict)):\n",
    "     save.append([str(i),str(predict[i])])\n",
    "save=pd.DataFrame(save)\n",
    "save.to_csv(path_or_buf='resheniye.csv', sep=',', index=False, header=['Id','y'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
