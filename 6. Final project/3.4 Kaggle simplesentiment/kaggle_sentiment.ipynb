{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\d.v.kulemin.UTMN\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\utils\\__init__.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sequence\n",
      "C:\\Users\\d.v.kulemin.UTMN\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "C:\\Users\\d.v.kulemin.UTMN\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\grid_search.py:14: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Mapping, namedtuple, Sized\n",
      "C:\\Users\\d.v.kulemin.UTMN\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\d.v.kulemin.UTMN\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.decomposition import NMF, TruncatedSVD\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn import grid_search\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('products_sentiment_train.tsv', sep='\\t', names=['reviews', 'label'])\n",
    "test = pd.read_csv('products_sentiment_test.tsv', sep='\\t', index_col=['Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2 . take around 10,000 640x480 pictures .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i downloaded a trial version of computer assoc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the wrt54g plus the hga7t is a perfect solutio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i dont especially like how music files are uns...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i was using the cheapie pail ... and it worked...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  label\n",
       "0          2 . take around 10,000 640x480 pictures .      1\n",
       "1  i downloaded a trial version of computer assoc...      1\n",
       "2  the wrt54g plus the hga7t is a perfect solutio...      1\n",
       "3  i dont especially like how music files are uns...      0\n",
       "4  i was using the cheapie pail ... and it worked...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>so , why the small digital elph , rather than ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3/4 way through the first disk we played on it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>better for the zen micro is outlook compatibil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6 . play gameboy color games on it with goboy .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>likewise , i 've heard norton 2004 professiona...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text\n",
       "Id                                                   \n",
       "0   so , why the small digital elph , rather than ...\n",
       "1   3/4 way through the first disk we played on it...\n",
       "2   better for the zen micro is outlook compatibil...\n",
       "3     6 . play gameboy color games on it with goboy .\n",
       "4   likewise , i 've heard norton 2004 professiona..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим размер обучающей выборки и долю положительных отзывов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.637"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train.label) / len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что положительных отзывов чуть больше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подбор модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переберем несколько методов извлечения признаков из текстов и классификаторов, выберем несколько наиболее удачных пайплайнов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_classifier(name_method_list):\n",
    "    return Pipeline([(name, method) for (name, method) in name_method_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizer: CountVectorizer, classifier: LogisticRegression\n",
      "0.7740071405738572\n",
      "vectorizer: CountVectorizer, classifier: LinearSVC\n",
      "0.7505076290683487\n",
      "vectorizer: CountVectorizer, classifier: SGDClassifier\n",
      "0.7350101225663445\n",
      "vectorizer: CountVectorizer, classifier: MultinomialNB\n",
      "0.7795006400703551\n",
      "vectorizer: CountVectorizer, classifier: RandomForestClassifier\n",
      "0.7194953574263919\n",
      "vectorizer: CountVectorizer, classifier: GradientBoostingClassifier\n",
      "0.7320033676855267\n",
      "vectorizer: TfidfVectorizer, classifier: LogisticRegression\n",
      "0.7575056315686001\n",
      "vectorizer: TfidfVectorizer, classifier: LinearSVC\n",
      "0.7685001343172257\n",
      "vectorizer: TfidfVectorizer, classifier: SGDClassifier\n",
      "0.7540021280650966\n",
      "vectorizer: TfidfVectorizer, classifier: MultinomialNB\n",
      "0.7005026015520768\n",
      "vectorizer: TfidfVectorizer, classifier: RandomForestClassifier\n",
      "0.7115038576807692\n",
      "vectorizer: TfidfVectorizer, classifier: GradientBoostingClassifier\n",
      "0.7185048616832725\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for vct, v_name in zip([CountVectorizer(), TfidfVectorizer()], \n",
    "                       ['CountVectorizer', 'TfidfVectorizer']):\n",
    "    for clf, c_name in zip([LogisticRegression(random_state=0), \n",
    "                            LinearSVC(random_state=0), \n",
    "                            SGDClassifier(max_iter=1000, random_state=0), \n",
    "                            MultinomialNB(), \n",
    "                            RandomForestClassifier(random_state=0), \n",
    "                            GradientBoostingClassifier(random_state=0)], \n",
    "                           ['LogisticRegression', \n",
    "                            'LinearSVC', \n",
    "                            'SGDClassifier', \n",
    "                            'MultinomialNB', \n",
    "                            'RandomForestClassifier', \n",
    "                            'GradientBoostingClassifier']):\n",
    "        score = cross_val_score(review_classifier([(v_name, vct), (c_name, clf)]), train.reviews, train.label).mean()\n",
    "        scores.append((f'{v_name} + {c_name}', score))\n",
    "        print(f'vectorizer: {v_name}, classifier: {c_name}')\n",
    "        print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizer: CountVectorizer, transformer: TfidfTransformer, classifier: LogisticRegression\n",
      "0.7575056315686001\n",
      "vectorizer: CountVectorizer, transformer: TfidfTransformer, classifier: LinearSVC\n",
      "0.7685001343172257\n",
      "vectorizer: CountVectorizer, transformer: TfidfTransformer, classifier: SGDClassifier\n",
      "0.7540021280650966\n",
      "vectorizer: CountVectorizer, transformer: TfidfTransformer, classifier: RandomForestClassifier\n",
      "0.7115038576807692\n",
      "vectorizer: CountVectorizer, transformer: TfidfTransformer, classifier: GradientBoostingClassifier\n",
      "0.7185048616832725\n",
      "vectorizer: CountVectorizer, transformer: NMF, classifier: LogisticRegression\n",
      "0.6644943294118707\n",
      "vectorizer: CountVectorizer, transformer: NMF, classifier: LinearSVC\n",
      "0.6985021003012007\n",
      "vectorizer: CountVectorizer, transformer: NMF, classifier: SGDClassifier\n",
      "0.698495346921134\n",
      "vectorizer: CountVectorizer, transformer: NMF, classifier: RandomForestClassifier\n",
      "0.6925208566887727\n",
      "vectorizer: CountVectorizer, transformer: NMF, classifier: GradientBoostingClassifier\n",
      "0.7250008629318975\n",
      "vectorizer: CountVectorizer, transformer: TruncatedSVD, classifier: LogisticRegression\n",
      "0.7514988751870311\n",
      "vectorizer: CountVectorizer, transformer: TruncatedSVD, classifier: LinearSVC\n",
      "0.7519956238097167\n",
      "vectorizer: CountVectorizer, transformer: TruncatedSVD, classifier: SGDClassifier\n",
      "0.7464991228109669\n",
      "vectorizer: CountVectorizer, transformer: TruncatedSVD, classifier: RandomForestClassifier\n",
      "0.6610043326685006\n",
      "vectorizer: CountVectorizer, transformer: TruncatedSVD, classifier: GradientBoostingClassifier\n",
      "0.7254968611790202\n",
      "vectorizer: TfidfVectorizer, transformer: TfidfTransformer, classifier: LogisticRegression\n",
      "0.7420066243154699\n",
      "vectorizer: TfidfVectorizer, transformer: TfidfTransformer, classifier: LinearSVC\n",
      "0.7614976295635966\n",
      "vectorizer: TfidfVectorizer, transformer: TfidfTransformer, classifier: SGDClassifier\n",
      "0.7455041248144697\n",
      "vectorizer: TfidfVectorizer, transformer: TfidfTransformer, classifier: RandomForestClassifier\n",
      "0.717002609806208\n",
      "vectorizer: TfidfVectorizer, transformer: TfidfTransformer, classifier: GradientBoostingClassifier\n",
      "0.7175031103067084\n",
      "vectorizer: TfidfVectorizer, transformer: NMF, classifier: LogisticRegression\n",
      "0.6369998184091137\n",
      "vectorizer: TfidfVectorizer, transformer: NMF, classifier: LinearSVC\n",
      "0.6759968364166266\n",
      "vectorizer: TfidfVectorizer, transformer: NMF, classifier: SGDClassifier\n",
      "0.6615055835445641\n",
      "vectorizer: TfidfVectorizer, transformer: NMF, classifier: RandomForestClassifier\n",
      "0.6854973414193805\n",
      "vectorizer: TfidfVectorizer, transformer: NMF, classifier: GradientBoostingClassifier\n",
      "0.7260063661862762\n",
      "vectorizer: TfidfVectorizer, transformer: TruncatedSVD, classifier: LogisticRegression\n",
      "0.737999618809214\n",
      "vectorizer: TfidfVectorizer, transformer: TruncatedSVD, classifier: LinearSVC\n",
      "0.7520001260630945\n",
      "vectorizer: TfidfVectorizer, transformer: TruncatedSVD, classifier: SGDClassifier\n",
      "0.7549993771882827\n",
      "vectorizer: TfidfVectorizer, transformer: TruncatedSVD, classifier: RandomForestClassifier\n",
      "0.677009093051072\n",
      "vectorizer: TfidfVectorizer, transformer: TruncatedSVD, classifier: GradientBoostingClassifier\n",
      "0.7259981120550836\n",
      "Wall time: 4min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for vct, v_name in zip([CountVectorizer(), TfidfVectorizer()], \n",
    "                       ['CountVectorizer', 'TfidfVectorizer']):\n",
    "    for trf, t_name in zip([TfidfTransformer(), NMF(n_components=100), TruncatedSVD(n_components=100)], \n",
    "                           ['TfidfTransformer', 'NMF', 'TruncatedSVD']):\n",
    "        for clf, c_name in zip([LogisticRegression(random_state=0), \n",
    "                                LinearSVC(random_state=0), \n",
    "                                SGDClassifier(max_iter=1000, random_state=0),  \n",
    "                                RandomForestClassifier(random_state=0), \n",
    "                                GradientBoostingClassifier(random_state=0)], \n",
    "                               ['LogisticRegression', \n",
    "                                'LinearSVC', \n",
    "                                'SGDClassifier', \n",
    "                                'RandomForestClassifier', \n",
    "                                'GradientBoostingClassifier']):\n",
    "            score = cross_val_score(review_classifier([(v_name, vct), (t_name, trf), (c_name, clf)]), train.reviews, train.label).mean()\n",
    "            scores.append((f'{v_name} + {t_name} + {c_name}', score))\n",
    "            print(f'vectorizer: {v_name}, transformer: {t_name}, classifier: {c_name}')\n",
    "            print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CountVectorizer + MultinomialNB', 0.7795006400703551),\n",
       " ('CountVectorizer + LogisticRegression', 0.7740071405738572),\n",
       " ('TfidfVectorizer + LinearSVC', 0.7685001343172257),\n",
       " ('CountVectorizer + TfidfTransformer + LinearSVC', 0.7685001343172257),\n",
       " ('TfidfVectorizer + TfidfTransformer + LinearSVC', 0.7614976295635966),\n",
       " ('TfidfVectorizer + LogisticRegression', 0.7575056315686001),\n",
       " ('CountVectorizer + TfidfTransformer + LogisticRegression',\n",
       "  0.7575056315686001),\n",
       " ('TfidfVectorizer + TruncatedSVD + SGDClassifier', 0.7549993771882827),\n",
       " ('TfidfVectorizer + SGDClassifier', 0.7540021280650966),\n",
       " ('CountVectorizer + TfidfTransformer + SGDClassifier', 0.7540021280650966),\n",
       " ('TfidfVectorizer + TruncatedSVD + LinearSVC', 0.7520001260630945),\n",
       " ('CountVectorizer + TruncatedSVD + LinearSVC', 0.7519956238097167),\n",
       " ('CountVectorizer + TruncatedSVD + LogisticRegression', 0.7514988751870311),\n",
       " ('CountVectorizer + LinearSVC', 0.7505076290683487),\n",
       " ('CountVectorizer + TruncatedSVD + SGDClassifier', 0.7464991228109669),\n",
       " ('TfidfVectorizer + TfidfTransformer + SGDClassifier', 0.7455041248144697),\n",
       " ('TfidfVectorizer + TfidfTransformer + LogisticRegression',\n",
       "  0.7420066243154699),\n",
       " ('TfidfVectorizer + TruncatedSVD + LogisticRegression', 0.737999618809214),\n",
       " ('CountVectorizer + SGDClassifier', 0.7350101225663445),\n",
       " ('CountVectorizer + GradientBoostingClassifier', 0.7320033676855267),\n",
       " ('TfidfVectorizer + NMF + GradientBoostingClassifier', 0.7260063661862762),\n",
       " ('TfidfVectorizer + TruncatedSVD + GradientBoostingClassifier',\n",
       "  0.7259981120550836),\n",
       " ('CountVectorizer + TruncatedSVD + GradientBoostingClassifier',\n",
       "  0.7254968611790202),\n",
       " ('CountVectorizer + NMF + GradientBoostingClassifier', 0.7250008629318975),\n",
       " ('CountVectorizer + RandomForestClassifier', 0.7194953574263919),\n",
       " ('TfidfVectorizer + GradientBoostingClassifier', 0.7185048616832725),\n",
       " ('CountVectorizer + TfidfTransformer + GradientBoostingClassifier',\n",
       "  0.7185048616832725),\n",
       " ('TfidfVectorizer + TfidfTransformer + GradientBoostingClassifier',\n",
       "  0.7175031103067084),\n",
       " ('TfidfVectorizer + TfidfTransformer + RandomForestClassifier',\n",
       "  0.717002609806208),\n",
       " ('TfidfVectorizer + RandomForestClassifier', 0.7115038576807692),\n",
       " ('CountVectorizer + TfidfTransformer + RandomForestClassifier',\n",
       "  0.7115038576807692),\n",
       " ('TfidfVectorizer + MultinomialNB', 0.7005026015520768),\n",
       " ('CountVectorizer + NMF + LinearSVC', 0.6985021003012007),\n",
       " ('CountVectorizer + NMF + SGDClassifier', 0.698495346921134),\n",
       " ('CountVectorizer + NMF + RandomForestClassifier', 0.6925208566887727),\n",
       " ('TfidfVectorizer + NMF + RandomForestClassifier', 0.6854973414193805),\n",
       " ('TfidfVectorizer + TruncatedSVD + RandomForestClassifier',\n",
       "  0.677009093051072),\n",
       " ('TfidfVectorizer + NMF + LinearSVC', 0.6759968364166266),\n",
       " ('CountVectorizer + NMF + LogisticRegression', 0.6644943294118707),\n",
       " ('TfidfVectorizer + NMF + SGDClassifier', 0.6615055835445641),\n",
       " ('CountVectorizer + TruncatedSVD + RandomForestClassifier',\n",
       "  0.6610043326685006),\n",
       " ('TfidfVectorizer + NMF + LogisticRegression', 0.6369998184091137)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.sort(key=lambda x: x[1], reverse=True)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стоит рассмотреть следующие пайплайны:\n",
    "\n",
    "* CountVectorizer + MultinomialNB\n",
    "* CountVectorizer + LogisticRegression\n",
    "* TfidfVectorizer + LinearSVC\n",
    "* TfidfVectorizer + TruncatedSVD + SGDClassifier\n",
    "* TfidfVectorizer + TfidfTransformer + LinearSVC\n",
    "* TfidfVectorizer + LogisticRegression\n",
    "\n",
    "Применим поиск параметров по сетке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_by_grid(X, y, model, parameters_grid, cv, scoring='accuracy', verbose=2):\n",
    "    grid_cv = GridSearchCV(model, parameters_grid, scoring=scoring, cv=cv, verbose=verbose)\n",
    "    grid_cv.fit(X, y)\n",
    "    return grid_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_by_models(X, y, grid_cv_params):\n",
    "    grid_cvs = []\n",
    "    for params in grid_cv_params:\n",
    "        grid_cvs.append(search_by_grid(X, y, *params))\n",
    "    return grid_cvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cv_params = [[Pipeline([('vectorizer', CountVectorizer()), ('classifier', MultinomialNB())]),\n",
    "                   {\n",
    "                        'vectorizer__analyzer': ['word', 'char', 'char_wb'],\n",
    "                        'vectorizer__stop_words': [None, 'english'],\n",
    "                        'vectorizer__ngram_range': [(i, j) for i in range(1, 4) for j in range(1, 4) if j >= i],\n",
    "                        'vectorizer__max_df': [0.3, 0.5, 0.8, 1.0],\n",
    "                        'classifier__alpha': [0.5, 0.7, 1.0, 10, 100],\n",
    "                        'classifier__fit_prior': [True, False]\n",
    "                    }, StratifiedKFold(), 'accuracy', 1],\n",
    "                  [Pipeline([('vectorizer', CountVectorizer()), ('classifier', LogisticRegression())]), \n",
    "                   {\n",
    "                        'vectorizer__analyzer': ['word', 'char', 'char_wb'],\n",
    "                        'vectorizer__stop_words': [None, 'english'],\n",
    "                        'vectorizer__ngram_range': [(i, j) for i in range(1, 4) for j in range(1, 4) if j >= i],\n",
    "                        'vectorizer__max_df': [0.3, 0.5, 0.8, 1.0],\n",
    "                        'classifier__C': [0.5, 0.7, 1.0, 10, 100, 1000],\n",
    "                        'classifier__penalty': ['l1', 'l2'],\n",
    "                        'classifier__solver': ['liblinear']\n",
    "                   }, StratifiedKFold(), 'accuracy', 1],\n",
    "                  [Pipeline([('vectorizer', CountVectorizer()), ('classifier', LogisticRegression(n_jobs=-1))]), \n",
    "                   {\n",
    "                        'vectorizer__analyzer': ['word', 'char', 'char_wb'],\n",
    "                        'vectorizer__stop_words': [None, 'english'],\n",
    "                        'vectorizer__ngram_range': [(i, j) for i in range(1, 4) for j in range(1, 4) if j >= i],\n",
    "                        'vectorizer__max_df': [0.3, 0.5, 0.8, 1.0],\n",
    "                        'classifier__C': [0.5, 0.7, 1.0, 10, 100, 1000],\n",
    "                        'classifier__penalty': ['l2'],\n",
    "                        'classifier__solver': ['newton-cg', 'lbfgs', 'sag', 'saga']\n",
    "                   }, StratifiedKFold(), 'accuracy', 1],\n",
    "                  [Pipeline([('vectorizer', TfidfVectorizer()), ('classifier', LinearSVC(random_state=0))]), \n",
    "                   {\n",
    "                        'vectorizer__analyzer': ['word', 'char', 'char_wb'],\n",
    "                        'vectorizer__stop_words': [None, 'english'],\n",
    "                        'vectorizer__ngram_range': [(i, j) for i in range(1, 4) for j in range(1, 4) if j >= i],\n",
    "                        'vectorizer__norm': ['l1', 'l2', None],\n",
    "                        'vectorizer__use_idf': [True, False],\n",
    "                        'vectorizer__max_df': [0.3, 0.5, 0.8, 1.0],\n",
    "                        'classifier__C': [0.5, 0.7, 1.0, 10, 100, 1000],\n",
    "                   }, StratifiedKFold(), 'accuracy', 1], \n",
    "                  [Pipeline([('vectorizer', TfidfVectorizer()), \n",
    "                             ('transformer', TruncatedSVD()), \n",
    "                             ('classifier', SGDClassifier(max_iter=1000, random_state=0, n_jobs=-1))]), \n",
    "                   {\n",
    "                        #'vectorizer__analyzer': ['word', 'char', 'char_wb'],\n",
    "                        'vectorizer__stop_words': [None, 'english'],\n",
    "                        'vectorizer__ngram_range': [(i, j) for i in range(1, 4) for j in range(1, 4) if j >= i],\n",
    "                        'vectorizer__norm': ['l1', 'l2', None],\n",
    "                        'vectorizer__use_idf': [True, False],\n",
    "                        'vectorizer__max_df': [0.3, 0.5, 0.8, 1.0],\n",
    "                        'transformer__n_components': [1, 2, 10, 100, 200],\n",
    "                        'classifier__penalty': ['l1', 'l2', None, 'elasticnet']\n",
    "                   }, StratifiedKFold(), 'accuracy', 1],\n",
    "                  [Pipeline([('vectorizer', TfidfVectorizer()), \n",
    "                             ('transformer', TfidfTransformer()), \n",
    "                             ('classifier', LinearSVC(random_state=0))]), \n",
    "                   {\n",
    "                        'vectorizer__analyzer': ['word', 'char', 'char_wb'],\n",
    "                        'vectorizer__stop_words': [None, 'english'],\n",
    "                        'vectorizer__ngram_range': [(i, j) for i in range(1, 4) for j in range(1, 4) if j >= i],\n",
    "                        'vectorizer__norm': ['l1', 'l2', None],\n",
    "                        'vectorizer__use_idf': [True, False],\n",
    "                        'vectorizer__max_df': [0.3, 0.5, 0.8, 1.0],\n",
    "                        'transformer__norm': ['l1', 'l2', None],\n",
    "                        'transformer__use_idf': [True, False],\n",
    "                        'classifier__C': [0.5, 0.7, 1.0, 10, 100, 1000],\n",
    "                   }, StratifiedKFold(), 'accuracy', 1],\n",
    "                  [Pipeline([('vectorizer', TfidfVectorizer()), ('classifier', LogisticRegression(random_state=0))]),\n",
    "                   {\n",
    "                        'vectorizer__analyzer': ['word', 'char', 'char_wb'],\n",
    "                        'vectorizer__stop_words': [None, 'english'],\n",
    "                        'vectorizer__ngram_range': [(i, j) for i in range(1, 4) for j in range(1, 4) if j >= i],\n",
    "                        'vectorizer__norm': ['l1', 'l2', None],\n",
    "                        'vectorizer__use_idf': [True, False],\n",
    "                        'vectorizer__max_df': [0.3, 0.5, 0.8, 1.0],\n",
    "                        'classifier__C': [0.5, 0.7, 1.0, 10, 100, 1000],\n",
    "                        'classifier__penalty': ['l1', 'l2'],\n",
    "                        'classifier__solver': ['liblinear']\n",
    "                   }, StratifiedKFold(), 'accuracy', 1], \n",
    "                  [Pipeline([('vectorizer', TfidfVectorizer()), ('classifier', LogisticRegression(random_state=0, n_jobs=-1))]),\n",
    "                   {\n",
    "                        'vectorizer__analyzer': ['word', 'char', 'char_wb'],\n",
    "                        'vectorizer__stop_words': [None, 'english'],\n",
    "                        'vectorizer__ngram_range': [(i, j) for i in range(1, 4) for j in range(1, 4) if j >= i],\n",
    "                        'vectorizer__norm': ['l1', 'l2', None],\n",
    "                        'vectorizer__use_idf': [True, False],\n",
    "                        'vectorizer__max_df': [0.3, 0.5, 0.8, 1.0],\n",
    "                        'classifier__C': [0.5, 0.7, 1.0, 10, 100, 1000],\n",
    "                        'classifier__penalty': ['l2'],\n",
    "                        'classifier__solver': ['newton-cg', 'lbfgs', 'sag', 'saga']\n",
    "                   }, StratifiedKFold(), 'accuracy', 1]\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ОСТОРОЖНО! выполнение следующей ячейки требует очень много времени. У меня выполнялась 1 день, 17 часов, 41 минуту!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1440 candidates, totalling 4320 fits\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "grid_cvs = search_by_models(train.reviews, train.label, grid_cv_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучшие модели:\n",
    "\n",
    "### CountVectorizer + MultinomialNB:\n",
    "\n",
    "Параметры\n",
    "\n",
    "* 'classifier__alpha': 1.0, \n",
    "* 'classifier__fit_prior': True, \n",
    "* 'vectorizer__analyzer': 'char_wb', \n",
    "* 'vectorizer__max_df': 1.0, \n",
    "* 'vectorizer__ngram_range': (3, 3), \n",
    "* 'vectorizer__stop_words': None\n",
    "\n",
    "Score: 0.784\n",
    "\n",
    "Score на тесте: 0.76888\n",
    "\n",
    "### CountVectorizer + LogisticRegression:\n",
    "\n",
    "* 'classifier__C': 1.0, \n",
    "* 'classifier__penalty': 'l2', \n",
    "* 'classifier__solver': 'liblinear', \n",
    "* 'vectorizer__analyzer': 'word', \n",
    "* 'vectorizer__max_df': 0.8, \n",
    "* 'vectorizer__ngram_range': (1, 1), \n",
    "* 'vectorizer__stop_words': None\n",
    "\n",
    "Score: 0.774\n",
    "\n",
    "### CountVectorizer + LogisticRegression:\n",
    "\n",
    "* 'classifier__C': 0.5, \n",
    "* 'classifier__penalty': 'l2', \n",
    "* 'classifier__solver': 'sag', \n",
    "* 'vectorizer__analyzer': 'word', \n",
    "* 'vectorizer__max_df': 0.8, \n",
    "* 'vectorizer__ngram_range': (1, 2), \n",
    "* 'vectorizer__stop_words': None\n",
    "\n",
    "Score: 0.7755\n",
    "\n",
    "Score на тесте: 0.81333\n",
    "\n",
    "### TfidfVectorizer + LinearSVC:\n",
    "\n",
    "* 'classifier__C': 100, \n",
    "* 'vectorizer__analyzer': 'word', \n",
    "* 'vectorizer__max_df': 0.3, \n",
    "* 'vectorizer__ngram_range': (1, 3), \n",
    "* 'vectorizer__norm': 'l1', \n",
    "* 'vectorizer__stop_words': None, \n",
    "* 'vectorizer__use_idf': True\n",
    "\n",
    "Score: 0.79\n",
    "\n",
    "#### Score на тесте: 0.81777\n",
    "\n",
    "### TfidfVectorizer + TruncatedSVD + SGDClassifier:\n",
    "\n",
    "* 'classifier__penalty': 'l2', \n",
    "* 'transformer__n_components': 200, \n",
    "* 'vectorizer__max_df': 0.5, \n",
    "* 'vectorizer__ngram_range': (1, 2), \n",
    "* 'vectorizer__norm': 'l2', \n",
    "* 'vectorizer__stop_words': None, \n",
    "* 'vectorizer__use_idf': True \n",
    "\n",
    "Score: 0.769\n",
    "\n",
    "Score на тесте: 0.76222\n",
    "\n",
    "### TfidfVectorizer + TfidfTransformer + LinearSVC:\n",
    "\n",
    "* 'classifier__C': 10, \n",
    "* 'transformer__norm': None, \n",
    "* 'transformer__use_idf': True, \n",
    "* 'vectorizer__analyzer': 'word', \n",
    "* 'vectorizer__max_df': 0.3, \n",
    "* 'vectorizer__ngram_range': (1, 3), \n",
    "* 'vectorizer__norm': 'l1', \n",
    "* 'vectorizer__stop_words': None, \n",
    "* 'vectorizer__use_idf': False \n",
    "\n",
    "Score: 0.792\n",
    "\n",
    "Score на тесте: 0.80888\n",
    "\n",
    "### TfidfVectorizer + LogisticRegression:\n",
    "\n",
    "* 'classifier__C': 1000, \n",
    "* 'classifier__penalty': 'l2', \n",
    "* 'classifier__solver': 'liblinear', \n",
    "* 'vectorizer__analyzer': 'word', \n",
    "* 'vectorizer__max_df': 0.3, \n",
    "* 'vectorizer__ngram_range': (1, 3), \n",
    "* 'vectorizer__norm': 'l2', \n",
    "* 'vectorizer__stop_words': None, \n",
    "* 'vectorizer__use_idf': True\n",
    "\n",
    "Score: 0.786\n",
    "\n",
    "#### Score на тесте: 0.82000\n",
    "\n",
    "### TfidfVectorizer + LogisticRegression:\n",
    "\n",
    "* 'classifier__C': 1000, \n",
    "* 'classifier__penalty': 'l2', \n",
    "* 'classifier__solver': 'sag', \n",
    "* 'vectorizer__analyzer': 'word', \n",
    "* 'vectorizer__max_df': 0.3, \n",
    "* 'vectorizer__ngram_range': (1, 3), \n",
    "* 'vectorizer__norm': 'l2', \n",
    "* 'vectorizer__stop_words': None, \n",
    "* 'vectorizer__use_idf': True \n",
    "\n",
    "Score: 0.786\n",
    "\n",
    "Score на тесте: 0.80888\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним все посчитанные модели в файл. Выполнение первой ячейки так же занимает существенное время."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\d.v.kulemin.UTMN\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\model_selection\\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\d.v.kulemin.UTMN\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\model_selection\\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\d.v.kulemin.UTMN\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\model_selection\\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\d.v.kulemin.UTMN\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\model_selection\\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\d.v.kulemin.UTMN\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\model_selection\\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\d.v.kulemin.UTMN\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\model_selection\\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\d.v.kulemin.UTMN\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\model_selection\\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\d.v.kulemin.UTMN\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\model_selection\\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 58min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_models = pd.DataFrame()\n",
    "for pipe, grid in zip(['CountVectorizer + MultinomialNB', \n",
    "                         'CountVectorizer + LogisticRegression', \n",
    "                         'CountVectorizer + LogisticRegression', \n",
    "                         'TfidfVectorizer + LinearSVC', \n",
    "                         'TfidfVectorizer + TruncatedSVD + SGDClassifier', \n",
    "                         'TfidfVectorizer + TfidfTransformer + LinearSVC', \n",
    "                         'TfidfVectorizer + LogisticRegression', \n",
    "                         'TfidfVectorizer + LogisticRegression'], grid_cvs):\n",
    "    for p in sorted(grid.grid_scores_, key=lambda x: x[1], reverse=True):\n",
    "        all_models = all_models.append(pd.DataFrame({'pileline': pipe, 'mean': p[1], **p[0]}), sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pileline</th>\n",
       "      <th>mean</th>\n",
       "      <th>classifier__alpha</th>\n",
       "      <th>classifier__fit_prior</th>\n",
       "      <th>vectorizer__analyzer</th>\n",
       "      <th>vectorizer__max_df</th>\n",
       "      <th>vectorizer__ngram_range</th>\n",
       "      <th>vectorizer__stop_words</th>\n",
       "      <th>classifier__C</th>\n",
       "      <th>classifier__penalty</th>\n",
       "      <th>classifier__solver</th>\n",
       "      <th>vectorizer__norm</th>\n",
       "      <th>vectorizer__use_idf</th>\n",
       "      <th>transformer__n_components</th>\n",
       "      <th>transformer__norm</th>\n",
       "      <th>transformer__use_idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CountVectorizer + MultinomialNB</td>\n",
       "      <td>0.784</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>char_wb</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CountVectorizer + MultinomialNB</td>\n",
       "      <td>0.784</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>char_wb</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CountVectorizer + MultinomialNB</td>\n",
       "      <td>0.784</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>char_wb</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>english</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CountVectorizer + MultinomialNB</td>\n",
       "      <td>0.784</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>char_wb</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>english</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CountVectorizer + MultinomialNB</td>\n",
       "      <td>0.782</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>char_wb</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          pileline   mean  classifier__alpha  \\\n",
       "0  CountVectorizer + MultinomialNB  0.784                1.0   \n",
       "1  CountVectorizer + MultinomialNB  0.784                1.0   \n",
       "0  CountVectorizer + MultinomialNB  0.784                1.0   \n",
       "1  CountVectorizer + MultinomialNB  0.784                1.0   \n",
       "0  CountVectorizer + MultinomialNB  0.782                1.0   \n",
       "\n",
       "  classifier__fit_prior vectorizer__analyzer  vectorizer__max_df  \\\n",
       "0                  True              char_wb                 1.0   \n",
       "1                  True              char_wb                 1.0   \n",
       "0                  True              char_wb                 1.0   \n",
       "1                  True              char_wb                 1.0   \n",
       "0                  True              char_wb                 0.5   \n",
       "\n",
       "   vectorizer__ngram_range vectorizer__stop_words  classifier__C  \\\n",
       "0                        3                   None            NaN   \n",
       "1                        3                   None            NaN   \n",
       "0                        3                english            NaN   \n",
       "1                        3                english            NaN   \n",
       "0                        3                   None            NaN   \n",
       "\n",
       "  classifier__penalty classifier__solver vectorizer__norm vectorizer__use_idf  \\\n",
       "0                 NaN                NaN              NaN                 NaN   \n",
       "1                 NaN                NaN              NaN                 NaN   \n",
       "0                 NaN                NaN              NaN                 NaN   \n",
       "1                 NaN                NaN              NaN                 NaN   \n",
       "0                 NaN                NaN              NaN                 NaN   \n",
       "\n",
       "   transformer__n_components transformer__norm transformer__use_idf  \n",
       "0                        NaN               NaN                  NaN  \n",
       "1                        NaN               NaN                  NaN  \n",
       "0                        NaN               NaN                  NaN  \n",
       "1                        NaN               NaN                  NaN  \n",
       "0                        NaN               NaN                  NaN  "
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_models.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models.to_csv('all_models.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models_upd = all_models.iloc[all_models.index == 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\d.v.kulemin.UTMN\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "all_models_upd['vectorizer__ngram_range'] = list(zip(all_models[all_models.index == 0].vectorizer__ngram_range, all_models[all_models.index == 1].vectorizer__ngram_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models_upd.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pileline</th>\n",
       "      <th>mean</th>\n",
       "      <th>classifier__alpha</th>\n",
       "      <th>classifier__fit_prior</th>\n",
       "      <th>vectorizer__analyzer</th>\n",
       "      <th>vectorizer__max_df</th>\n",
       "      <th>vectorizer__ngram_range</th>\n",
       "      <th>vectorizer__stop_words</th>\n",
       "      <th>classifier__C</th>\n",
       "      <th>classifier__penalty</th>\n",
       "      <th>classifier__solver</th>\n",
       "      <th>vectorizer__norm</th>\n",
       "      <th>vectorizer__use_idf</th>\n",
       "      <th>transformer__n_components</th>\n",
       "      <th>transformer__norm</th>\n",
       "      <th>transformer__use_idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CountVectorizer + MultinomialNB</td>\n",
       "      <td>0.784</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>char_wb</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CountVectorizer + MultinomialNB</td>\n",
       "      <td>0.784</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>char_wb</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>english</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CountVectorizer + MultinomialNB</td>\n",
       "      <td>0.782</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>char_wb</td>\n",
       "      <td>0.5</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CountVectorizer + MultinomialNB</td>\n",
       "      <td>0.782</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>char_wb</td>\n",
       "      <td>0.5</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>english</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CountVectorizer + MultinomialNB</td>\n",
       "      <td>0.782</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>char_wb</td>\n",
       "      <td>0.8</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          pileline   mean  classifier__alpha  \\\n",
       "0  CountVectorizer + MultinomialNB  0.784                1.0   \n",
       "1  CountVectorizer + MultinomialNB  0.784                1.0   \n",
       "2  CountVectorizer + MultinomialNB  0.782                1.0   \n",
       "3  CountVectorizer + MultinomialNB  0.782                1.0   \n",
       "4  CountVectorizer + MultinomialNB  0.782                1.0   \n",
       "\n",
       "  classifier__fit_prior vectorizer__analyzer  vectorizer__max_df  \\\n",
       "0                  True              char_wb                 1.0   \n",
       "1                  True              char_wb                 1.0   \n",
       "2                  True              char_wb                 0.5   \n",
       "3                  True              char_wb                 0.5   \n",
       "4                  True              char_wb                 0.8   \n",
       "\n",
       "  vectorizer__ngram_range vectorizer__stop_words  classifier__C  \\\n",
       "0                  (3, 3)                   None            NaN   \n",
       "1                  (3, 3)                english            NaN   \n",
       "2                  (3, 3)                   None            NaN   \n",
       "3                  (3, 3)                english            NaN   \n",
       "4                  (3, 3)                   None            NaN   \n",
       "\n",
       "  classifier__penalty classifier__solver vectorizer__norm vectorizer__use_idf  \\\n",
       "0                 NaN                NaN              NaN                 NaN   \n",
       "1                 NaN                NaN              NaN                 NaN   \n",
       "2                 NaN                NaN              NaN                 NaN   \n",
       "3                 NaN                NaN              NaN                 NaN   \n",
       "4                 NaN                NaN              NaN                 NaN   \n",
       "\n",
       "   transformer__n_components transformer__norm transformer__use_idf  \n",
       "0                        NaN               NaN                  NaN  \n",
       "1                        NaN               NaN                  NaN  \n",
       "2                        NaN               NaN                  NaN  \n",
       "3                        NaN               NaN                  NaN  \n",
       "4                        NaN               NaN                  NaN  "
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_models_upd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Файл с моделями приложу к ноутбуку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models_upd.to_csv('all_models_upd.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим предсказания для лучших моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 302 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for pipe, grid in zip(['CountVectorizer + MultinomialNB', \n",
    "                         'CountVectorizer + LogisticRegression(liblinear)', \n",
    "                         'CountVectorizer + LogisticRegression', \n",
    "                         'TfidfVectorizer + LinearSVC', \n",
    "                         'TfidfVectorizer + TruncatedSVD + SGDClassifier', \n",
    "                         'TfidfVectorizer + TfidfTransformer + LinearSVC', \n",
    "                         'TfidfVectorizer + LogisticRegression(liblinear)', \n",
    "                         'TfidfVectorizer + LogisticRegression'], grid_cvs):\n",
    "    answer = pd.DataFrame({'Id': test.index, 'y': grid.best_estimator_.predict(test.text)})\n",
    "    answer.to_csv(f'submition {pipe}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучший результат получился у модели TfidfVectorizer + LogisticRegression:\n",
    "\n",
    "* 'classifier__C': 1000,\n",
    "* 'classifier__penalty': 'l2',\n",
    "* 'classifier__solver': 'liblinear',\n",
    "* 'vectorizer__analyzer': 'word',\n",
    "* 'vectorizer__max_df': 0.3,\n",
    "* 'vectorizer__ngram_range': (1, 3),\n",
    "* 'vectorizer__norm': 'l2',\n",
    "* 'vectorizer__stop_words': None,\n",
    "* 'vectorizer__use_idf': True\n",
    "\n",
    "Score: 0.786\n",
    "\n",
    "Score на тесте: 0.82000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По состоянию на 09.09.2019 с этой моделью получилось подняться на 10 строчку:\n",
    "\n",
    "![title](score.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Больше данных!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея: можно попробовать добавить к нашим сэмплам несколько случайно выбранных сэмплов из той же выборки с некоторым шумом в признаках. Возможно так получится улучшить качество модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(matrix, labels, pos, neg, feature_noise='count', feature_range=range(1, 11), \n",
    "          val_range_count=range(1, 3), val_range_tfidf=np.arange(0.0001, 0.01, 0.001)):\n",
    "    pos_indices = [i for i, value in enumerate(labels) if value == 1]\n",
    "    neg_indices = [i for i, value in enumerate(labels) if value == 0]\n",
    "    feature_indices = list(range(matrix.shape[1]))\n",
    "    for idx in sorted(np.append(np.random.choice(pos_indices, pos), np.random.choice(neg_indices, neg))):\n",
    "        labels.append(labels[idx])\n",
    "        row = matrix[idx].toarray()[0]\n",
    "        for i in sorted(np.random.choice(feature_indices, np.random.choice(feature_range))):\n",
    "            if feature_noise == 'count':\n",
    "                row[i] += np.random.choice(val_range_count)\n",
    "            elif feature_noise == 'tfidf':\n",
    "                row[i] += np.random.choice(val_range_tfidf)\n",
    "        matrix = sparse.vstack((matrix, sparse.csr_matrix(row[np.newaxis, :])))\n",
    "    return sparse.csr_matrix(matrix), labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Провери две разные более-менее удачные модели:\n",
    "\n",
    "TfidfVectorizer + LogisticRegression:\n",
    "\n",
    "* 'classifier__C': 1000,\n",
    "* 'classifier__penalty': 'l2',\n",
    "* 'classifier__solver': 'liblinear',\n",
    "* 'vectorizer__analyzer': 'word',\n",
    "* 'vectorizer__max_df': 0.3,\n",
    "* 'vectorizer__ngram_range': (1, 3),\n",
    "* 'vectorizer__norm': 'l2',\n",
    "* 'vectorizer__stop_words': None,\n",
    "* 'vectorizer__use_idf': True\n",
    "\n",
    "Score: 0.786\n",
    "\n",
    "Score на тесте: 0.82000\n",
    "\n",
    "Score на тесте +2548: 0.81777\n",
    "\n",
    "CountVectorizer + LogisticRegression:\n",
    "\n",
    "* 'classifier__C': 1.0, \n",
    "* 'classifier__penalty': 'l2', \n",
    "* 'classifier__solver': 'liblinear', \n",
    "* 'vectorizer__analyzer': 'word', \n",
    "* 'vectorizer__max_df': 0.8, \n",
    "* 'vectorizer__ngram_range': (1, 1), \n",
    "* 'vectorizer__stop_words': None\n",
    "\n",
    "Score: 0.774\n",
    "\n",
    "Score на тесте: 0.81333\n",
    "\n",
    "Score на тесте +2548: 0.78888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeliner(vect_name, vectorizer, clf_name, classifier, train_X, train_y, test_X, pos, neg, feature_noise='count', \n",
    "              feature_range=range(1, 11), val_range_count=range(1, 3), val_range_tfidf=np.arange(0.0001, 0.01, 0.001)):\n",
    "    train_features = vectorizer.fit_transform(train_X, train_y)\n",
    "    test_features = vectorizer.transform(test_X)\n",
    "    matrix, labels = noise(train_features, list(train_y), pos, neg, feature_noise=feature_noise, \n",
    "                           feature_range=feature_range, val_range_count=val_range_count, \n",
    "                           val_range_tfidf=val_range_tfidf)\n",
    "    classifier.fit(matrix, labels)\n",
    "    answer = pd.DataFrame({'Id': test.index, 'y': classifier.predict(test_features)})\n",
    "    answer.to_csv(f'submition {vect_name} + {clf_name}(+{pos + neg}).csv', index=False)\n",
    "    return cross_val_score(classifier, matrix, labels).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помним, что выборка не сбалансирована, добавим разное количество позитивных и негативных отзывов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9696569920844328"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeliner('TfidfVectorizer', TfidfVectorizer(max_df=0.3, ngram_range=(1, 3)), \n",
    "          'LogisticRegression', LogisticRegression(C=1000, random_state=0), \n",
    "          train.reviews, train.label, test.text, 1000, 1548, feature_noise='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8733509234828496"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeliner('CountVectorizer', CountVectorizer(max_df=0.8, ngram_range=(1, 1)), \n",
    "          'LogisticRegression', LogisticRegression(random_state=0), \n",
    "          train.reviews, train.label, test.text, 1000, 1548, feature_noise='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Знаем, что так как модель с TfidfVectorizer использует униграммы, биграммы и триграммы, поэтому изменим диапазон выбранных изменяемых признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9696569920844328"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeliner('TfidfVectorizer', TfidfVectorizer(max_df=0.3, ngram_range=(1, 3)), \n",
    "          'LogisticRegression@', LogisticRegression(C=1000, random_state=0), \n",
    "          train.reviews, train.label, test.text, 1000, 1548, feature_noise='tfidf', feature_range=range(10, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что на кросс-валидации получаем многообещающее качество. К сожалению на тесте качество падает, выбор другого диапазона изменяемых признаков так же снизило качество модели до 0.81555. Видимо произошло переобучение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы:\n",
    "\n",
    "1. Очень хорошо показали себя линейные классификаторы, в то время как ансамбли и Наивный Байесовский классификатор выдали качество хуже.\n",
    "2. Интересно, что в лучший моделях не было стоп слов.\n",
    "3. Синтезированные данные не дали улучшения в качестве на тесте, хотя на обучении качество росло. Очевидно эффект переобучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
